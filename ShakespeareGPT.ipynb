{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wDl8lpCzWB5J",
        "outputId": "d3fcc2fc-18d7-4958-9840-126f1f5bd9d6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-19 06:51:06--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "input.txt           100%[===================>]   1.06M  --.-KB/s    in 0.009s  \n",
            "\n",
            "2023-11-19 06:51:07 (118 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUMxFxfIWJF9",
        "outputId": "3134274b-167f-4859-8d5f-78f970b35776"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2237, val loss 4.2234\n",
            "step 500: train loss 1.6321, val loss 1.6345\n",
            "step 1000: train loss 1.3041, val loss 1.3091\n",
            "step 1500: train loss 1.1714, val loss 1.1793\n",
            "step 2000: train loss 1.0665, val loss 1.0762\n",
            "step 2500: train loss 0.9612, val loss 0.9722\n",
            "step 3000: train loss 0.8472, val loss 0.8597\n",
            "step 3500: train loss 0.7281, val loss 0.7437\n",
            "step 4000: train loss 0.6130, val loss 0.6293\n",
            "step 4500: train loss 0.5090, val loss 0.5271\n",
            "step 5000: train loss 0.4188, val loss 0.4382\n",
            "step 5500: train loss 0.3496, val loss 0.3616\n",
            "step 6000: train loss 0.2918, val loss 0.3056\n",
            "step 6500: train loss 0.2438, val loss 0.2516\n",
            "step 7000: train loss 0.2137, val loss 0.2218\n",
            "step 7500: train loss 0.1908, val loss 0.1972\n",
            "step 8000: train loss 0.1726, val loss 0.1783\n",
            "step 8500: train loss 0.1591, val loss 0.1630\n",
            "step 9000: train loss 0.1492, val loss 0.1529\n",
            "step 9500: train loss 0.1406, val loss 0.1441\n",
            "step 10000: train loss 0.1351, val loss 0.1377\n",
            "step 10500: train loss 0.1294, val loss 0.1319\n",
            "step 11000: train loss 0.1241, val loss 0.1262\n",
            "step 11500: train loss 0.1224, val loss 0.1245\n",
            "step 12000: train loss 0.1174, val loss 0.1193\n",
            "step 12500: train loss 0.1145, val loss 0.1164\n",
            "step 13000: train loss 0.1132, val loss 0.1147\n",
            "step 13500: train loss 0.1103, val loss 0.1122\n",
            "step 14000: train loss 0.1082, val loss 0.1099\n",
            "step 14500: train loss 0.1074, val loss 0.1083\n",
            "step 14999: train loss 0.1051, val loss 0.1062\n",
            "\n",
            "ere their true king's\n",
            "Own with rafe and wiltter from their spen.\n",
            "\n",
            "Second Watchman:\n",
            "Whom thou children, with the king his kindness;\n",
            "And he who fails of the house of Lancaster.\n",
            "\n",
            "First Watchman:\n",
            "A great suspicion: say the troubles will thee?\n",
            "\n",
            "Second Musician:\n",
            "Faith, I would speak thee well.\n",
            "\n",
            "LEONTES:\n",
            "I thought of her, be sure, if thou wilt remove\n",
            "the custion.\n",
            "\n",
            "PERDITA:\n",
            "For a shame!\n",
            "The thing is thy boar with this extremity;\n",
            "Watching raging with thy virtues, the ends\n",
            "O'er addstrate to him in an hue the state\n",
            "And beat did me asled asleep; then redeem hence,\n",
            "It is a kind of his unlawful bed;\n",
            "And hung amazed many flights, do fail,\n",
            "Upon an assault to distress him fhere and fairly;\n",
            "And when such more veives, that is as true\n",
            "When I am as like thinking stail came to my Lucentio.\n",
            "\n",
            "LUCIO:\n",
            "Sir, my lord, here is Lucentio.\n",
            "\n",
            "MIONESTRESS OVERDONE:\n",
            "Go, be then;\n",
            "For what you have bided look'd fareigon sorrow.\n",
            "I pray you, if I had a shame wonder,\n",
            "If well be my caudity.\n",
            "\n",
            "First Service:\n",
            "Or, if you do so, go beyond to your chear?\n",
            "\n",
            "Second Citizen:\n",
            "What would you do?\n",
            "\n",
            "CORIOLANUS:\n",
            "Should this, a undissing one friend in the speech,\n",
            "Which sounded? I would not you for the world\n",
            "He were well at Tunis. Ladies your honours\n",
            "Made by my hearts; and I oftend my servant would\n",
            "upon the head open will after the tomb;\n",
            "For haste a triple bed unto my common peech\n",
            "And the shepherd: where, as I thought was a\n",
            "commission, and I confessoed to under my bausposom myself,\n",
            "And I fortune my father many man's live.\n",
            "\n",
            "FLORIZEL:\n",
            "And I, belike, and that my foe.\n",
            "\n",
            "PROSPERO:\n",
            "Do so, both; and after thee.\n",
            "\n",
            "AUTOLYCUS:\n",
            "Proceed it me; you are heart: the proudest of the belly;\n",
            "but I have better'd by my acts and myself.\n",
            "\n",
            "MARCIUS:\n",
            "Here's a friar won; I cannot speak: the\n",
            "pray receping for home, would I wash abuse one place of\n",
            "the courageous had they been bolted, and near wash their fatains\n",
            "difsed outruproes; slaves take drunks reems them!--\n",
            "O, if you love the bride the herdsmer down.\n",
            "\n",
            "MARCIUS:\n",
            "Because they love their beds that dare; be king, but quy,\n",
            "Thou violent friendly as enclosed a prison.\n",
            "\n",
            "SLY:\n",
            "Yes, by your honour's pardon, that he hath touch'd upon\n",
            "The deserts of Bushy.\n",
            "\n",
            "GLOUCESTER:\n",
            "What news is dead!\n",
            "\n",
            "LADY CAPULET:\n",
            "Alas, Warwick, did the hope of these new mames?\n",
            "\n",
            "KING EDWARD IV:\n",
            "Is your wit the blood of such a parlor's face?\n",
            "Richard, thou livest; and all the veins else\n",
            "When now springs can faith, all the day is lost of hell.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Tell him, or both: I protest I like it not?\n",
            "\n",
            "KING HENRY VI:\n",
            "Be patient, then, to age this house tomb;\n",
            "Whilst thou livest thy heaviness will acquittance more.\n",
            "What, in my stricks mischanging heaven, that forgot you\n",
            "And that my abster servant, as you say, pro\n",
            "Far this most round and her holding her pains:\n",
            "I will, and throw his wife and there with much,\n",
            "Until my hand attain'd entered in my heart,\n",
            "Thou seek'st my heart of ancient in it;\n",
            "Tears thy happiness and thy groans.\n",
            "\n",
            "KING RICHARD II:\n",
            "Marshal, ask yonder knight in heapt of mine.\n",
            "\n",
            "kift Watchman:\n",
            "Beseech you, Clarence, did I some comfort from\n",
            "The proSon of City Paliting Baptista's master;\n",
            "us, in all his presences, so harpily\n",
            "He is note with outward.\n",
            "\n",
            "Provost:\n",
            "A man, or I, and therefore pardon me,\n",
            "A pripe compassion do touch in the violence;\n",
            "On his beard, as I for lawful hence,\n",
            "Let a beauty of moiety more for than myself,\n",
            "That no poor Isabel, give me leave to queen.\n",
            "\n",
            "DUCHESS OF YORK:\n",
            "\n",
            "HENOND:\n",
            "Would you have been sick, dear and mine health speak:\n",
            "Be high the Duke of Norfolk's gance,\n",
            "And of the French hath kept in an Yeork,\n",
            "And bade the happy to England;\n",
            "And by his combating Rutland, slain amber'd,\n",
            "My landed knows, I for that brought should it return?\n",
            "No, now commend, how the blodge thing he wave;\n",
            "For wearing were with the morn, thou wert to eyes.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "And when the king stay and he was indeed\n",
            "Had left it steam'd his tombs, like a hand\n",
            "Where it doth the former King of France.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "Was princely Buckingham, I would I to God-morrow\n",
            "Hath turn the news in deliver volume of her\n",
            "With diverse brother there living liege one word:\n",
            "Lonely, how fares your princely father York\n",
            "Blung Edward is now a day to his way:\n",
            "The royaltiest I love my wife, and thankswer not;\n",
            "My army of tribes yet toward his husband\n",
            "And now by myself myself alone.\n",
            "\n",
            "ANGELOBETH:\n",
            "Welcome, myself! why, is here.\n",
            "What's a matter which promised the danting:\n",
            "Comes I to be but at London, come again,\n",
            "Self here, poest King Henry and Dical;\n",
            "And that is King Edward in scornspring of me.\n",
            "\n",
            "CLARENCE:\n",
            "As well as Lewis of France, of that news\n",
            "Hath the Sicils steal led abour me?\n",
            "\n",
            "KING EDWARD IV:\n",
            "Nay, then shows me put what he abides himself,\n",
            "As begind the envious assurance never falling.\n",
            "\n",
            "GLOUCESTER:\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "And long live that out so minster as you.\n",
            "\n",
            "LORD BERKELEY:\n",
            "My lord, for that cannot there be remorseful,\n",
            "His bed-chrown his dissol in peace.\n",
            "\n",
            "HASTINGS:\n",
            "What, with like grief, make no less proud march\n",
            "To meet the shop Tybalt by danger therein\n",
            "Like labour to consent but the revel of that\n",
            "That loss the time of Jove have foundA a Gaunt,\n",
            "And all the maiden plain King Lewith keeps.\n",
            "Best them special, that for their time merry.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "To Edwards, to the unking crown is dead.\n",
            "\n",
            "YORK:\n",
            "Why will you pluck as if you did blance from the earth.\n",
            "\n",
            "KING LEWARD IV:\n",
            "Then know you will, and tell the world truppe\n",
            "the sea the stable he has becomety.\n",
            "\n",
            "PROSPERO:\n",
            "Common more, and thou poisonous mind: thou dost seest in\n",
            "The sky Tarlous roser that a wench in the Topman is state,\n",
            "And ailst upon a sudden bride.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "Come, I am passing wish a fresh strew,\n",
            "Because I would be the gentle straint all arms;\n",
            "Cry woe, and the sun-visitation strokes,\n",
            "That is made to die, and do you purpose,\n",
            "Lent in Saint Japin. Thou'rt came to thee\n",
            "An art most mirth to heaven and me speak:\n",
            "I do beseech thee; thou art sure, torment thy\n",
            "state a brother not there. Do your time; and\n",
            "do well under sail, or I'll we make you parcel with\n",
            "this corn, and here I that you love think one\n",
            "now.\n",
            "\n",
            "Clown:\n",
            "Here comes the lady aspect of his highness' peace.\n",
            "\n",
            "ADRIAN:\n",
            "And the most sad lip person was might have mongled\n",
            "more than my daughtersed steed, which many habit,\n",
            "But at Oxfords the multic be foundamed.\n",
            "\n",
            "BUCKINGHAM:\n",
            "This is comfort; no more I better where.\n",
            "\n",
            "KING EDWARD IV:\n",
            "He is the matter where a man? Wherefore do you know\n",
            "He should have pass'd: for honourable 'mand,\n",
            "Bound to her and study passage life behold\n",
            "One for anour action: he's a gentleman\n",
            "Make an apt of their song, not for one removed\n",
            "As often being pardon'd, am come and play'd,\n",
            "Both of nothing knaver'd.\n",
            "\n",
            "PROSPERO:\n",
            "For hand, sir!\n",
            "I pray thee,\n",
            "Once it the more of other for a spirit,\n",
            "Or as he that been the favour-success;\n",
            "Or never after a beasts thou sure Katharina,\n",
            "Because I know, my lords, to grave the citizens.\n",
            "Upon Clarence, command mend hence to Bolingbroke,\n",
            "And all these I take my words as and yours,\n",
            "Supsmicied me your grace and my fortune rogue,\n",
            "To avoint me to be consul.\n",
            "\n",
            "BRUTUS:\n",
            "The gods have.\n",
            "\n",
            "SICINIUS: we should deserve, and they well meet.\n",
            "\n",
            "BRUTUS:\n",
            "I our possessible here to endure the people's eyes;\n",
            "But in the rotten of my servant here:\n",
            "Where Oxfords and his spirit, if renowned\n",
            "Were well contented with welcome home:\n",
            "And pray you promised me.\n",
            "\n",
            "Cirse:\n",
            "Know you need not to me, what and you that?\n",
            "\n",
            "CORIOLANUS:\n",
            "First, an it like you the sin that you love.\n",
            "\n",
            "VIRGILIA:\n",
            "No, none of Thursday we entites take my leave.\n",
            "Save's sake, Roges save a liege, and at once more;\n",
            "Then treasons may better than the master.\n",
            "\n",
            "CORIOLANUS:\n",
            "Sir, I have charge thee,\n",
            "Where grants have my heart stripling eyes mine heart;\n",
            "Witness whilst I with them, till I would be still\n",
            "Am from the talker the sand see the touch of the\n",
            "diademends: but there to the gods know his haste.\n",
            "\n",
            "GRUMIO:\n",
            "Within me to know a house, my lords, and answer now;\n",
            "That his longest have knowledge flows for his that:\n",
            "And being now thou, fair some woulds assured\n",
            "That Gentleman hath made as true to war\n",
            "This night had not been balms to pict me,\n",
            "They'll woo'd the way.\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "Rest take her off, friends, till amends mark.\n",
            "\n",
            "EDWARD:\n",
            "Even in the shape I send to die?\n",
            "\n",
            "QUEEN MARGARET:\n",
            "Ay, in my hand stooping hours and the Duke of York.\n",
            "\n",
            "YORK:\n",
            "I think King he is gone; but he got and dared not:\n",
            "I hope the king is some broke; but he,\n",
            "And be not holp with an eye to be extrail.\n",
            "\n",
            "BUCKINGHAM:\n",
            "He is your grace, given great with conscience sum.\n",
            "\n",
            "QUEEN ELIZABETH:\n",
            "To catch losence catch my in soldiers,\n",
            "And than to harm sounded his brother sway,\n",
            "Her suit is now a strange and a minim\n",
            "A most lickly govern'd at himself,\n",
            "To have a match.\n",
            "\n",
            "GLOUCESTER:\n",
            "Away me to the queen heart come not on.\n",
            "\n",
            "PRINCES:\n",
            "Come, Warwick, thou art wont to tell?\n",
            "\n",
            "WARWICK:\n",
            "If thou a king, which quots in peace and afflictic me\n",
            "To comfort me and for unhappy with G?\n",
            "It is sunless first being ready,\n",
            "And I would be hanged and my servant as you:\n",
            "When, well, we see, and hear him.\n",
            "\n",
            "BUCKINGHAM:\n",
            "Trust me, my lord, this are the king.\n",
            "\n",
            "GLOUCESTER:\n",
            "Welcome, dear comes me to the Duke of York.\n",
            "\n",
            "PRINCE EDWARD:\n",
            "An in placticle haste remembrance him,\n",
            "The doom of the beastle, callarent, come take from whence.\n",
            "Brother, the people; who was I obtain'd,\n",
            "That I should snarl at the private of actions,\n",
            "As are appeareth belonging to do least,\n",
            "When he doth suffer come to a brave burshen,\n",
            "And now but what you have done with a miscarce,\n",
            "Where fair for his ensueaveyage that places\n",
            "March'd the blood of fighting his asbitracle!\n",
            "Only you have found issued with a prince's son;\n",
            "Much promise were met stopp'd by him; but\n",
            "Your royal shoulder and the remedies\n",
            "But that small sooner recued.\n",
            "\n",
            "ADRIAN:\n",
            "Peace, peace,\n",
            "O Marialo!\n",
            "\n",
            "PROSPERO:\n",
            "Thou speak'st true that I should die?\n",
            "\n",
            "ARIEL:\n",
            "Do not swear\n",
            "At oft him, my dear.\n",
            "\n",
            "PROSPERO:\n",
            "\n",
            "MIRANDA:\n",
            "Now be well\n",
            "Speak from you acception.\n",
            "\n",
            "MIRANDA:\n",
            "No, no, nor your deliver:\n",
            "Love me at the sweethearOn,\n",
            "Do as minister.\n",
            "\n",
            "PROSPERO:\n",
            "Hence! hang not or sons:\n",
            "Now what never I will be there, tarry;\n",
            "One despair, in the place of power, the touch'd,\n",
            "He partake no venom, ' surcerent peers here.'\n",
            "\n",
            "EXETER:\n",
            "What, for my daughter Katha\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 15000\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.05\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)         # remove if want to get different outputs everytime\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = text + text\n",
        "text = text + text\n",
        "text = text + text\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rogFgUPGWPLk"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
