{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oA8DRYakYnJh",
        "outputId": "5ee8ae79-24be-48fd-bf18-242f02451e34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "--2023-11-25 07:31:32--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.111.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.111.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt’\n",
            "\n",
            "\rinput.txt             0%[                    ]       0  --.-KB/s               \rinput.txt           100%[===================>]   1.06M  --.-KB/s    in 0.06s   \n",
            "\n",
            "2023-11-25 07:31:32 (17.3 MB/s) - ‘input.txt’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "! wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OcjipwvhY-4_",
        "outputId": "3cb13d99-3d20-4ae1-ee46-1d977ea54885"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10.788929 M parameters\n",
            "step 0: train loss 4.2237, val loss 4.2234\n",
            "step 500: train loss 1.6321, val loss 1.6345\n",
            "step 1000: train loss 1.3041, val loss 1.3091\n",
            "step 1500: train loss 1.1714, val loss 1.1793\n",
            "step 2000: train loss 1.0665, val loss 1.0762\n",
            "step 2500: train loss 0.9612, val loss 0.9722\n",
            "step 3000: train loss 0.8472, val loss 0.8597\n",
            "step 3500: train loss 0.7281, val loss 0.7437\n",
            "step 4000: train loss 0.6130, val loss 0.6293\n",
            "step 4500: train loss 0.5090, val loss 0.5271\n",
            "step 5000: train loss 0.4188, val loss 0.4382\n",
            "step 5500: train loss 0.3496, val loss 0.3616\n",
            "step 6000: train loss 0.2918, val loss 0.3056\n",
            "step 6500: train loss 0.2438, val loss 0.2516\n",
            "step 7000: train loss 0.2137, val loss 0.2218\n",
            "step 7500: train loss 0.1908, val loss 0.1972\n",
            "step 8000: train loss 0.1726, val loss 0.1783\n",
            "step 8500: train loss 0.1591, val loss 0.1630\n",
            "step 9000: train loss 0.1492, val loss 0.1529\n",
            "step 9500: train loss 0.1406, val loss 0.1441\n",
            "step 10000: train loss 0.1351, val loss 0.1377\n",
            "step 10500: train loss 0.1294, val loss 0.1319\n",
            "step 11000: train loss 0.1241, val loss 0.1262\n",
            "step 11500: train loss 0.1224, val loss 0.1245\n",
            "step 12000: train loss 0.1174, val loss 0.1193\n",
            "step 12500: train loss 0.1145, val loss 0.1164\n",
            "step 13000: train loss 0.1132, val loss 0.1147\n",
            "step 13500: train loss 0.1103, val loss 0.1122\n",
            "step 14000: train loss 0.1082, val loss 0.1099\n",
            "step 14500: train loss 0.1074, val loss 0.1083\n",
            "step 15000: train loss 0.1051, val loss 0.1062\n",
            "step 15500: train loss 0.1023, val loss 0.1038\n",
            "step 16000: train loss 0.1010, val loss 0.1024\n",
            "step 16500: train loss 0.1003, val loss 0.1021\n",
            "step 17000: train loss 0.0996, val loss 0.1009\n",
            "step 17499: train loss 0.0974, val loss 0.0991\n",
            "\n",
            "DUKE VINCENTIO:\n",
            "ConveniTed.\n",
            "\n",
            "NORTEGOLIO:\n",
            "We will see to-morrow.\n",
            "\n",
            "LUCIO:\n",
            "This may past for them.\n",
            "\n",
            "ESCALUS:\n",
            "Say you lie that 'twere one that Tunis.\n",
            "\n",
            "ELBOW:\n",
            "Come your ways, sir; come.\n",
            "\n",
            "POMPEY:\n",
            "You will not bail me, then, she will be true.\n",
            "\n",
            "LUCIO:\n",
            "She's in the right.\n",
            "\n",
            "ISABELLA:\n",
            "That's Accuses, by Sigeia tellus; for we'll\n",
            "I lean to hear ye the skyey curb, being the\n",
            "preserved bookic of any ill-beehal'd,\n",
            "And now by the rebels where he have was letter the\n",
            "horse of the secrets, it was more sure in the\n",
            "time, they did love the thresheses of the person, he\n",
            "with the men out thet to be three at the earth-to\n",
            "of our over-rough; whose honour and how see\n",
            "you so oftend home?\n",
            "\n",
            "BRUTUS:\n",
            "Our willes enough too?\n",
            "\n",
            "MENENIUS:\n",
            "On false of that, the consul of the world:\n",
            "That we are the county lord of you be\n",
            "The at the brat, with a time offence disager,\n",
            "Begot with the duke, destribute and ben\n",
            "The comparent of the king nor mad\n",
            "Had not rather to my want of joy:\n",
            "So fit she'er us, and so say I will;\n",
            "'Scept both she'll not be say 'I'ld snot weep;' I'ld follow\n",
            "No better place the uncles, she shall :\n",
            "So many misery had marter there it\n",
            "Than my poor Jove's willine.\n",
            "\n",
            "COMINIUS:\n",
            "She should this Mowbray's wife\n",
            "As Caius Caius Marcius Coriolanus,\n",
            "Alone to the earth that did should have heard ths:\n",
            "Some say the lark, thou thing is for removedents.\n",
            "\n",
            "SICINIUS:\n",
            "Say, these you have this voices?\n",
            "\n",
            "Messenger:\n",
            "The news is mine own, if you be a pleasure;\n",
            "If you find a tinder of service to be\n",
            "monstrous and process of grace and for me\n",
            "But only counterfeit of what of her was born\n",
            "And not unavoided incured, we might die countenance\n",
            "And such a power as yourself was believed it.\n",
            "\n",
            "CLAUDIO:\n",
            "The precedent was grown born by him; for he were\n",
            "not troubled with a dastred contempt subdued;\n",
            "but what a shame, whate's a foolish dove,\n",
            "for serve best a vow, a subject of thy troth,\n",
            "and that thy supply reoved some of breath\n",
            "With a threw choler racks of love thee!\n",
            "My heart and gentle women are so comely,\n",
            "And shows thee as these a reverence and gold:\n",
            "There thou art a soldier of thy will\n",
            "to the charge; but where, I let thee France was saw.\n",
            "Thou factious queen, and for the looks of life:\n",
            "Mark'd by looks that thou reach thus heaving from steel,\n",
            "And feed thy souls thou shalt not hear me in stange.\n",
            "But if thou couldst not, thou consel me know,\n",
            "If thou be sign in so confited, true,\n",
            "Shalt the word of many friends that bones\n",
            "Be thou and topen him to this gain.\n",
            "\n",
            "ISABELLA:\n",
            "O pretty debt!\n",
            "I would you have made what is in the number:\n",
            "by the death of any commit there,\n",
            "To bar if thou couldst please between where list\n",
            "Thy year loyalty of happier and the\n",
            "Call it paint and merry as the wind.\n",
            "What you now?\n",
            "\n",
            "CLARENCE:\n",
            "That's a news abroad: as the great Alcais Elbow;\n",
            "Her suit is Anne that you comes toward you as\n",
            "you as pening in nothingable, nor\n",
            "Corowards not with your eving laws.\n",
            "\n",
            "AEdile:\n",
            "What shall you have served by him?\n",
            "\n",
            "GREMIO:\n",
            "And what have we heard since it is depossed?\n",
            "\n",
            "TRANIO:\n",
            "Tell him his not warn in the Tiber! What take him, bride\n",
            "He now stroke at widows betide th encour complaints;\n",
            "Hath not as the effect I to be done;\n",
            "Your commands and my fellow to open me.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "Kind Richard, look upon Edward's peace!\n",
            "Speak sweetly, Richard, do thou hear suddenly soul;\n",
            "For thou shalt know the reason of her mother's fault,\n",
            "And then be six ms is new-rise and strew,\n",
            "And straight shall doth his care and fly to ride\n",
            "This hand, fathom with a tumble instruction, first\n",
            "Were men mind of seventeen pounds; of whom I all assured\n",
            "Will not yet put on. What comes he not?\n",
            "Your provost knock whence to Flater Salisbury?\n",
            "\n",
            "EALL OW:\n",
            "To your own cause; it doth not so,\n",
            "You cannot give now.\n",
            "\n",
            "DION:\n",
            "Not patience, sir,\n",
            "Take it the way: a boy, and yours like to accept home.\n",
            "\n",
            "AUTOLYCUS:\n",
            "\n",
            "ESCALUS:\n",
            "Your and tittle, sir; where, on Tunis, the Earl of Marcius;\n",
            "Come not the duke's banishment; and when we are\n",
            "Almost their way to death.\n",
            "\n",
            "CAPULET:\n",
            "O Montagne, will you this true?\n",
            "\n",
            "LADY CAPULET:\n",
            "A fortnight and after if he deserved me,\n",
            "To make him a deep until a famour.\n",
            "\n",
            "CAPULET:\n",
            "So shalt thou return? for the sun Art there,\n",
            "Whom did seem to flight: tell them what of it\n",
            "horself we are throw: the bestle of eit in\n",
            "the son that sends a banish'd from the world.\n",
            "\n",
            "LUCIO:\n",
            "I pray thee, more than did that hath the word, being\n",
            "in my more resolve of death.\n",
            "\n",
            "CURTIS:\n",
            "Go to, good of you, take her nose.\n",
            "\n",
            "GRUMIO:\n",
            "What, you be at leisure: if you love her, I do it not,\n",
            "And that the sent of brothers have well\n",
            "As ever treators for death, which by the viol\n",
            "I am so longly on you: but which enjoy to-morrow\n",
            "Somet's forefather off; your Margaret's name too iscale.\n",
            "\n",
            "Nurse:\n",
            "Rome, sir; it is it in Greeting Richard;\n",
            "And you may my be, if he were sing a secondLaurer:\n",
            "She shall be in mine act Angelo.\n",
            "\n",
            "Behold:\n",
            "Why, here is the prison: treshepherdhate to you\n",
            "should be a couble for the maiden pour of your good counsel:\n",
            "How now, my mother! What is the matter?\n",
            "\n",
            "CLARENCE:\n",
            "A lion foul is he; little to make\n",
            "Her French seek, to o'erboth the scrook of her wall:\n",
            "Her hence with the crown disdain, like a true cheeks\n",
            "Crost-hock, hath suddens of war supplius--\n",
            "Saced down, Clifford! thou art slain,\n",
            "With thine eyes, see what showers up in our butchers.\n",
            "O sweet way, served well well at Franced strike\n",
            "And the Lady Bona stir Rivers, with all of wraths.\n",
            "Come, gentle sir, come on: what with thy word?\n",
            "\n",
            "PETRUCHIO:\n",
            "Master, lead me be both too much adders:\n",
            "Now thou to rejoice in thy love and low;\n",
            "The on thy art fortunation shall reat,\n",
            "The sacred honour of thy company.\n",
            "Is supposed the majesty always from thee,\n",
            "And with these shows a forerity whole,\n",
            "Thyself it determinable struches and hath\n",
            "A cause of his babe, and ever fellow took\n",
            "With some few words of that every crept\n",
            "Have left dream'd the gentle bow's blood, and full of eir a\n",
            "soul to his qualit?\n",
            "\n",
            "MENENIUS:\n",
            "He should not be long; therefore he should have gone\n",
            "The people against have held me been.\n",
            "\n",
            "First Senator:\n",
            "Come, come, we away.\n",
            "\n",
            "Second Senator:\n",
            "Do you hear how we are shent for keeping your\n",
            "greatness back?\n",
            "\n",
            "Second Senator:\n",
            "What cause, do you think, I hope he's warrant you, and\n",
            "if gently house to an indifferent we doubt.\n",
            "\n",
            "CORIOLANUS:\n",
            "What mean to you?\n",
            "\n",
            "SICINIUS:\n",
            "This is a hand, sir, that you are.\n",
            "\n",
            "CORIOLANUS:\n",
            "The present death of battle, the conquerors.\n",
            "\n",
            "Third Servant:\n",
            "Or else the princes, lord of the realm of thee.\n",
            "\n",
            "Second Servingman:\n",
            "'Tis so: to he deserves your business for me; he\n",
            "prays that the kind very night by day.\n",
            "\n",
            "First Servingman:\n",
            "What answer made the price of his face soul heart\n",
            "To his love that are hope in the compare\n",
            "Of her whom wounded.\n",
            "\n",
            "CORIOLANUS:\n",
            "Dear breathe I have sever'd my life of beteems\n",
            "A vessel assal thou art poor better this hour.\n",
            "\n",
            "BRUTUS:\n",
            "Lay\n",
            "And thank you, condemned us to the proudest law\n",
            "That their leads a dearer branchief\n",
            "To close a world of heart: the when we watch the king,\n",
            "That all the under hath been to bed.\n",
            "I'll not endurily where you lack on, if she being\n",
            "As may be you a call:\n",
            "Ah, were the vice shall not be so young on\n",
            "The faults of her from all the church didrences,\n",
            "yet see how to utterate his enemies!\n",
            "\n",
            "KING RICHARD II:\n",
            "Norfolk, we have brought a benefit\n",
            "Of great Heart with the eager and himself:\n",
            "And, in , form thine own live, a royal power,\n",
            "That ever, ere I reqwent to your for turn,\n",
            "And being the butchers of his blind for them.\n",
            "\n",
            "MIRANDA:\n",
            "O, thou fatily!\n",
            "The dog of thing, where lies shall be her liard,\n",
            "The fresh spectacles in my breast strength,\n",
            "The blood would it for be an hour agor:\n",
            "Perhaps you will have it ere you have redemember.\n",
            "\n",
            "JOHN OF GAUNT:\n",
            "I will between us the cause, my little good use in,\n",
            "To look on no the chamber of the pares.\n",
            "\n",
            "ROMEO:\n",
            "Is it even so?\n",
            "Thou didst it exceeding the rest thing oak\n",
            "Which thou shalt know the just preserve to his heart.\n",
            "\n",
            "DERBY:\n",
            "I see joy? by it is she made proveth out love.\n",
            "\n",
            "CLIFFORD:\n",
            "I have no treason: speak all the world.\n",
            "\n",
            "RICHARD:\n",
            "Brother, good old; for I send the about to have.\n",
            "\n",
            "TYBALT:\n",
            "Why, there is for my body's mind bow like a sobrie.\n",
            "\n",
            "TRANIO:\n",
            "And then, boy. the basta: be't assured\n",
            "Sometime an else bold to woo.\n",
            "\n",
            "MIRANDA:\n",
            "O horse that I tender\n",
            "You bid me kill him? Woe lions and kiss\n",
            "A big in him, a foe, and a near matter.\n",
            "\n",
            "LUCIO:\n",
            "I spake by the tribunes for the people, for a little,\n",
            "Because his something ready stretch at him.\n",
            "Now, afore God, nor that I may speak,\n",
            "My life speed hell mine eye and to thee\n",
            "That art thou myself on thy bed:\n",
            "Come, belike Henry, we, the warlike of the remods;\n",
            "How calrusiors and lowly in tongue,\n",
            "That words make him speak a foul glory tongue.\n",
            "Still will you perceive it shortly.\n",
            "\n",
            "DUKE OF AUMERLE:\n",
            "Come, come, my insolence; where they stand enough.\n",
            "\n",
            "HENRY BOLINGBROKE:\n",
            "How do you saist, as do the duke for reproof\n",
            "Do wrong with a glass, of whom you are\n",
            "All service as best by one rooked bank\n",
            "For trust at our bitter with mas, we'll some obey part.\n",
            "\n",
            "LUCIO:\n",
            "I grant; and ye'er he would not ever fell the world\n",
            "In Lancaster Dice. But here it , from the rest,\n",
            "More than the report of his supple agains.\n",
            "\n",
            "First Servant:\n",
            "Ay, sir, desperately; for all men in all,\n",
            "I'll do it frame and light.\n",
            "\n",
            "Second Servant:\n",
            "Ay, yet or be gone. \n",
            "\n",
            "First Soldier:\n",
            "So have I heard\n",
            "'twas pretty for all the night she side,\n",
            "And with them death like enough at ours.\n",
            "\n",
            "Second Servingman:\n",
            "And till then I have rescued intelligence\n",
            "That had send the heart to his charge nor heard of him\n",
            "to the dog of it. Sir, profanest, as we looked a\n",
            "full-doorols; that it were here the dead bold nor we at all.\n",
            "\n",
            "Third Servingman:\n",
            "What is thy master for thee?\n",
            "\n",
            "First Servingman:\n",
            "And these elements of worman are far off death,\n",
            "and over the medition.\n",
            "\n",
            "Second Servingman:\n",
            "What are you then?\n",
            "\n",
            "First Servingman:\n",
            "A man hour here been stumber, and troop his sweet;\n",
            "and, after me, I throw thee dead as free thee\n",
            "From thy kindom and not shalt think, thou keeper,\n",
            "Thou callest me tears a piece of strange satr.\n",
            "\n",
            "KING RICHARD III:\n",
            "Nor no man that sine agains by whilst you have\n",
            "An if to know our strive now weeds death in refor me.\n",
            "What tale more beholding to the pursue,\n",
            "Even so shortly, for her score and tears and she\n",
            "Thou \n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "# hyperparameters\n",
        "batch_size = 64 # how many independent sequences will we process in parallel?\n",
        "block_size = 256 # what is the maximum context length for predictions?\n",
        "max_iters = 17500\n",
        "eval_interval = 500\n",
        "learning_rate = 3e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 384\n",
        "n_head = 6\n",
        "n_layer = 6\n",
        "dropout = 0.05\n",
        "# ------------\n",
        "\n",
        "torch.manual_seed(1337)         # remove if want to get different outputs everytime\n",
        "\n",
        "# wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()\n",
        "\n",
        "text = text + text\n",
        "text = text + text\n",
        "text = text + text\n",
        "\n",
        "# here are all the unique characters that occur in this text\n",
        "chars = sorted(list(set(text)))\n",
        "vocab_size = len(chars)\n",
        "# create a mapping from characters to integers\n",
        "stoi = { ch:i for i,ch in enumerate(chars) }\n",
        "itos = { i:ch for i,ch in enumerate(chars) }\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: ''.join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n",
        "\n",
        "# Train and test splits\n",
        "data = torch.tensor(encode(text), dtype=torch.long)\n",
        "n = int(0.9*len(data)) # first 90% will be train, rest val\n",
        "train_data = data[:n]\n",
        "val_data = data[n:]\n",
        "\n",
        "# data loading\n",
        "def get_batch(split):\n",
        "    # generate a small batch of data of inputs x and targets y\n",
        "    data = train_data if split == 'train' else val_data\n",
        "    ix = torch.randint(len(data) - block_size, (batch_size,))\n",
        "    x = torch.stack([data[i:i+block_size] for i in ix])\n",
        "    y = torch.stack([data[i+1:i+block_size+1] for i in ix])\n",
        "    x, y = x.to(device), y.to(device)\n",
        "    return x, y\n",
        "\n",
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "    out = {}\n",
        "    model.eval()\n",
        "    for split in ['train', 'val']:\n",
        "        losses = torch.zeros(eval_iters)\n",
        "        for k in range(eval_iters):\n",
        "            X, Y = get_batch(split)\n",
        "            logits, loss = model(X, Y)\n",
        "            losses[k] = loss.item()\n",
        "        out[split] = losses.mean()\n",
        "    model.train()\n",
        "    return out\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(block_size, block_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # input of size (batch, time-step, channels)\n",
        "        # output of size (batch, time-step, head size)\n",
        "        B,T,C = x.shape\n",
        "        k = self.key(x)   # (B,T,hs)\n",
        "        q = self.query(x) # (B,T,hs)\n",
        "        # compute attention scores (\"affinities\")\n",
        "        wei = q @ k.transpose(-2,-1) * k.shape[-1]**-0.5 # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "        wei = wei.masked_fill(self.tril[:T, :T] == 0, float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        # perform the weighted aggregation of the values\n",
        "        v = self.value(x) # (B,T,hs)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, hs) -> (B, T, hs)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "\n",
        "    def __init__(self, num_heads, head_size):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(head_size * num_heads, n_embd)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "        out = self.dropout(self.proj(out))\n",
        "        return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class Block(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        # n_embd: embedding dimension, n_head: the number of heads we'd like\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "        self.sa = MultiHeadAttention(n_head, head_size)\n",
        "        self.ffwd = FeedFoward(n_embd)\n",
        "        self.ln1 = nn.LayerNorm(n_embd)\n",
        "        self.ln2 = nn.LayerNorm(n_embd)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.sa(self.ln1(x))\n",
        "        x = x + self.ffwd(self.ln2(x))\n",
        "        return x\n",
        "\n",
        "class GPTLanguageModel(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        # each token directly reads off the logits for the next token from a lookup table\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(block_size, n_embd)\n",
        "        self.blocks = nn.Sequential(*[Block(n_embd, n_head=n_head) for _ in range(n_layer)])\n",
        "        self.ln_f = nn.LayerNorm(n_embd) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size)\n",
        "\n",
        "        # better init, not covered in the original GPT video, but important, will cover in followup video\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, module):\n",
        "        if isinstance(module, nn.Linear):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "            if module.bias is not None:\n",
        "                torch.nn.init.zeros_(module.bias)\n",
        "        elif isinstance(module, nn.Embedding):\n",
        "            torch.nn.init.normal_(module.weight, mean=0.0, std=0.02)\n",
        "\n",
        "    def forward(self, idx, targets=None):\n",
        "        B, T = idx.shape\n",
        "\n",
        "        # idx and targets are both (B,T) tensor of integers\n",
        "        tok_emb = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table(torch.arange(T, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        x = self.blocks(x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "            loss = None\n",
        "        else:\n",
        "            B, T, C = logits.shape\n",
        "            logits = logits.view(B*T, C)\n",
        "            targets = targets.view(B*T)\n",
        "            loss = F.cross_entropy(logits, targets)\n",
        "\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, max_new_tokens):\n",
        "        # idx is (B, T) array of indices in the current context\n",
        "        for _ in range(max_new_tokens):\n",
        "            # crop idx to the last block_size tokens\n",
        "            idx_cond = idx[:, -block_size:]\n",
        "            # get the predictions\n",
        "            logits, loss = self(idx_cond)\n",
        "            # focus only on the last time step\n",
        "            logits = logits[:, -1, :] # becomes (B, C)\n",
        "            # apply softmax to get probabilities\n",
        "            probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "            # sample from the distribution\n",
        "            idx_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "            # append sampled index to the running sequence\n",
        "            idx = torch.cat((idx, idx_next), dim=1) # (B, T+1)\n",
        "        return idx\n",
        "\n",
        "model = GPTLanguageModel()\n",
        "m = model.to(device)\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# create a PyTorch optimizer\n",
        "optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)\n",
        "\n",
        "for iter in range(max_iters):\n",
        "\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "        losses = estimate_loss()\n",
        "        print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb = get_batch('train')\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = model(xb, yb)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "# generate from the model\n",
        "context = torch.zeros((1, 1), dtype=torch.long, device=device)\n",
        "print(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))\n",
        "#open('more.txt', 'w').write(decode(m.generate(context, max_new_tokens=10000)[0].tolist()))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_vlUNnhoZD6x"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
